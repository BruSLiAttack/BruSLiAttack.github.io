## Welcome to GitHub Pages of BruSLiAttack

Reproduce our results: [GitHub](https://github.com/BruSLiAttack/BruSLiAttack.github.io)

Check out our paper: [BruSLiAttack: Bayesian Algorithm for Query-Efficient Score Based Sparse Attacks Against Black-Box Deep Learning Models](https://...)

Poster: [Poster](...)

#### ABSTRACT

The potential for extracting information, solely from the output of a machine learning model, poses safety and security threats against real-world systems; especially concerning in an era of proliferating models offered as Machine Learning as a Service (MLaaS). Sparse attacks are of particular interest. Because, they aim to discover the minimum number of perturbations to model inputs—l0 bounded perturbations—to craft adversarial examples to misguide model decisions, and expose a unique class of hidden model vulnerabilities. But, constructing sparse adversarial perturbations without prior model knowledge, against black-box models, even when models opt to serve confidence score information to queries—in a score-based attack setting—is non-trivial. Because, such an attack leads to: i) an NP-hard problem; and a ii) non-differentiable search space. We develop an algorithm built upon a Bayesian framework for the problem and evaluate against Convolutional deep Neural Networks (CNN), Vision Transformers (ViT) and recent Stylized ImageNet models (SIN). Importantly, vision transformers are yet to be investigated under a score-based attack setting. The attack demonstrate significantly high attack success rates with low query budgets; on the high-resolution ImageNet, 10 K queries achieve over 98% attack success rate against a CNN model, 87% against a ViT model and 97% against a SIN model in the hard setting of an attack to a target class with just 1% sparsity. Importantly, the highly query efficient algorithm we demonstrate to discover hidden model weaknesses raises new questions regarding the safety of deployed systems and the robustness of machine learning models.

#### Demonstration on Google Cloud Vision
![Figure 1](images/gcv-example-stop sign.svg#gh-dark-mode-only)

Figure 1: a) A source image is classified as _Traffic_ Sign by __Google Cloud Vision (GCV)__. b) With less than _500 queries_, __BruS-LiAttack__ is able to yield a sparsity adversarial example (250 of 50,176 pixels are manipulated) misclassified as _Tree_ by GCV.

#### Illustration of Sparse Adversarial Examples

![Figure 2](images/imagenet-adv ex-visualization.svg#gh-dark-mode-only)

Figure  2: Targeted Attack. Malicious instances is generated by BruSLiAttack with different perturbation budgets against different Deep Learning models on ImageNet. An image with ground-truth label Minibus is misclassified as a Warplane. Interestingly, BruSLi-Attack requires 80 perturbed pixels (over 50,176 pixels) to fool ResNet-based models whereas it has to manipulate 220 pixels to mislead Vision Transformer

![Figure 3](images/GCV demo table.svg#gh-dark-mode-only)
Table 1: Demonstration of sparse attacks against GCV in targeted settings. BruSLiAttack is able to successfully yield adversarial instances for all five examples with less queries than SparseRS. Especially, for the example of Mushroom, SparseRS fails to attack GCV within a budget of 5000 queries. Demonstration on GCV API (online platform).

![Figure 4](images/gcv-demonstration.svg#gh-dark-mode-only)
Figure  3: a) demonstrates results for clean image (no attack) predicted by Google Cloud Vision (GCV). b) shows the predictions from GCV for adversarial examples crafted successfully by BruSLiAttack with less than 3,000 queries and sparsity of 0.05 %. c) shows the results from GCV for adversarial examples crafted by SparseRS with the same sparsity. But SparseRS needs more queries than BruSLiAttack to successfully yield adversarial images or fail to attack with query budget up to 5,000 as shown in Tab. 1
