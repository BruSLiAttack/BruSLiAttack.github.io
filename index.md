## Welcome to GitHub Pages of BruSLiAttack

Reproduce our results: [GitHub](https://github.com/BruSLiAttack/BruSLiAttack.github.io)

Check out our paper: [BruSLiAttack: Bayesian Algorithm for Query-Efficient Score Based Sparse Attacks Against Black-Box Deep Learning Models](https://...)

Poster: [Poster](...)

[Clean and perturbed samples & artifacts](https://github.com/BruSLiAttack/BruSLiAttack.github.io/tree/main/artifacts) can be downloaded and tested on [Google Cloud Vision](https://cloud.google.com/vision) (Test them for free!!!)

Cite our research: 
```
@inproceedings{Anonymous...,
    title = {BruSLiAttack: Bayesian Algorithm for Query-Efficient Score Based Sparse Attacks Against Black-Box Deep Learning Models},
    year = {...},
    journal = {...},
    author = Anonymous},
}
```

#### ABSTRACT

The potential for extracting information, solely from the output of a machine learning model, poses safety and security threats against real-world systems; especially concerning in an era of proliferating models offered as Machine Learning as a Service (MLaaS). Sparse attacks are of particular interest. Because they aim to discover the minimum number of perturbations to model inputs—l0 bounded perturbations—to craft adversarial examples to misguide model decisions and expose a unique class of hidden model vulnerabilities. But, constructing sparse adversarial perturbations without prior model knowledge, even when models opt to serve confidence score information to queries—in a score-based attack setting—is non-trivial. Because such an attack leads to: i) an NP-hard problem; and ii) a nondifferentiable search space. We develop BruSLiAttack built upon a Bayesian framework for the problem and evaluate against Convolutional deep Neural Networks (CNN), Vision Transformers (ViT) and recent Stylized ImageNet models (SIN). With 10 K queries, our attack can achieve over 98% attack success rate against a CNN model, 87% against a ViT model and 97% against a SIN model on the high-resolution ImageNet and in a targeted setting with just 1% sparsity. Importantly, our highly query-efficient algorithm demonstrates hidden model weaknesses and raises questions regarding the safety of deployed systems. 

#### Demonstration on Google Cloud Vision
![Figure 1](figures/gcv-example-stop sign.svg#gh-dark-mode-only)

Figure 1: a) A source image is classified as _Traffic_ Sign by __Google Cloud Vision (GCV)__. b) With less than _500 queries_, __BruS-LiAttack__ is able to yield a sparsity adversarial example (250 of 50,176 pixels are manipulated) misclassified as _Tree_ by GCV.

#### Illustration of Sparse Adversarial Examples

![Figure 2](figures/imagenet-adv ex-visualization.svg#gh-dark-mode-only)

Figure  2: Targeted Attack. Malicious instances is generated by BruSLiAttack with different perturbation budgets against different Deep Learning models on ImageNet. An image with ground-truth label Minibus is misclassified as a Warplane. Interestingly, BruSLi-Attack requires 80 perturbed pixels (over 50,176 pixels) to fool ResNet-based models whereas it has to manipulate 220 pixels to mislead Vision Transformer.
