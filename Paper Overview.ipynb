{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to GitHub Pages of BruSLiAttack\n",
    "\n",
    "Reproduce our results: [GitHub](https://github.com/BruSLiAttack/BruSLiAttack.github.io)\n",
    "\n",
    "Check out our paper: [BruSLiAttack: Bayesian Algorithm for Query-Efficient Score Based Sparse Attacks Against Black-Box Deep Learning Models](https://...)\n",
    "\n",
    "Poster: [Poster](...)\n",
    "\n",
    "[Clean and perturbed samples & artifacts](https://github.com/BruSLiAttack/BruSLiAttack.github.io/tree/main/artifacts) can be downloaded and tested on [Google Cloud Vision](https://cloud.google.com/vision) (Test them for free!!!)\n",
    "\n",
    "Cite our research: \n",
    "```\n",
    "@inproceedings{Anonymous...,\n",
    "    title = {BruSLiAttack: Bayesian Algorithm for Query-Efficient Score Based Sparse Attacks Against Black-Box Deep Learning Models},\n",
    "    year = {...},\n",
    "    journal = {...},\n",
    "    author = Anonymous},\n",
    "}\n",
    "```\n",
    "\n",
    "#### ABSTRACT\n",
    "\n",
    "The potential for extracting information, solely from the output of a machine learning model, poses safety and security threats against real-world systems; especially concerning in an era of proliferating models offered as Machine Learning as a Service (MLaaS). Sparse attacks are of particular interest. Because they aim to discover the minimum number of perturbations to model inputs—l0 bounded perturbations—to craft adversarial examples to misguide model decisions and expose a unique class of hidden model vulnerabilities. But, constructing sparse adversarial perturbations without prior model knowledge, even when models opt to serve confidence score information to queries—in a score-based attack setting—is non-trivial. Because such an attack leads to: i) an NP-hard problem; and ii) a nondifferentiable search space. We develop BruSLiAttack built upon a Bayesian framework for the problem and evaluate against Convolutional deep Neural Networks (CNN), Vision Transformers (ViT) and recent Stylized ImageNet models (SIN). With 10 K queries, our attack can achieve over 98% attack success rate against a CNN model, 87% against a ViT model and 97% against a SIN model on the high-resolution ImageNet and in a targeted setting with just 1% sparsity. Importantly, our highly query-efficient algorithm demonstrates hidden model weaknesses and raises questions regarding the safety of deployed systems. \n",
    "\n",
    "#### Demonstration on Google Cloud Vision\n",
    "![Figure 1](figures/gcv-example-stop sign.svg)\n",
    "<img src=\".\\figures\\gcv-example-stop sign.svg\"/>\n",
    "\n",
    "Figure 1: a) A source image is classified as _Traffic_ Sign by __Google Cloud Vision (GCV)__. b) With less than _500 queries_, __BruS-LiAttack__ is able to yield a sparsity adversarial example (250 of 50,176 pixels are manipulated) misclassified as _Tree_ by GCV.\n",
    "\n",
    "#### Illustration of Sparse Adversarial Examples\n",
    "\n",
    "![Figure 2](figures/imagenet-adv ex-visualization.svg)\n",
    "<img src=\"figures/imagenet-adv ex-visualization.svg\"/>\n",
    "\n",
    "Figure  2: Targeted Attack. Malicious instances is generated by BruSLiAttack with different perturbation budgets against different Deep Learning models on ImageNet. An image with ground-truth label Minibus is misclassified as a Warplane. Interestingly, BruSLi-Attack requires 80 perturbed pixels (over 50,176 pixels) to fool ResNet-based models whereas it has to manipulate 220 pixels to mislead Vision Transformer.\n",
    "\n",
    "![Figure 3](figures/GCV demo table.svg)\n",
    "\n",
    "Table 1: Demonstration of sparse attacks against GCV in targeted settings. BruSLiAttack is able to successfully yield adversarial instances for all five examples with less queries than SparseRS. Especially, for the example of Mushroom, SparseRS fails to attack GCV within a budget of 5000 queries. Demonstration on GCV API (online platform).\n",
    "\n",
    "![Figure 3](figures/adv ex-imagenet-different arch-visualization.svg#gh-dark-mode-only)\n",
    "\n",
    "Figure  3: Targeted Attack. Visualization of Adversarial examples crafted by BruSLiAttack with a budget of 5000 queries.\n",
    "\n",
    "![Figure 3b](figures/adv ex-imagenet-different arch-visualization-2.svg#gh-dark-mode-only)\n",
    "\n",
    "![Figure 4a](figures/gcv-demonstration.svg)\n",
    "\n",
    "Figure  4: a) demonstrates results for clean image (no attack) predicted by Google Cloud Vision (GCV). b) shows the predictions from GCV for adversarial examples crafted successfully by BruSLiAttack with less than 3,000 queries and sparsity of 0.05 %. c) shows the results from GCV for adversarial examples crafted by SparseRS with the same sparsity. But SparseRS needs more queries than BruSLiAttack to successfully yield adversarial images or fail to attack with query budget up to 5,000 as shown in Tab. 1.\n",
    "\n",
    "![Figure 4c](figures/gcv-demonstration-2.svg#gh-dark-mode-only)\n",
    "![Figure 4b](figures/GCV demo table-2.svg#gh-dark-mode-only)\n",
    "\n",
    "Figure  5: More examples for demonstration of the sparse adversarial instances yielded by __BruSLiAttack__ against a real-world model on __Google Cloud Vision__ platform.\n",
    "\n",
    "![Figure 5b](figures/Visualization of Dissimilarity Maps-2.svg#gh-dark-mode-only)\n",
    "\n",
    "\n",
    "![Figure 6](figures/Visualization of Dissimilarity Maps.svg)\n",
    "\n",
    "Figure  6: Visualization of Dissimilarity Maps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
